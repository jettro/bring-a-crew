{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe03986-b7d5-48a6-b038-5e8e8cbb88d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Recap of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc3862-b89f-4afe-b015-39b4d41fe13a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Learn the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9c069-bc93-4c0e-82b7-644bc89ed117",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": "## Recap LLMs ~ Generate an answer to a single question"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "from bring_a_crew.setup_logging import setup_logging\n",
    "from ollama import chat, generate\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5a00dc2-e1c4-4e1d-9648-24f914124c4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "MODEL = \"phi4\"\n",
    "\n",
    "_ = load_dotenv()\n",
    "setup_logging()\n",
    "main_log = logging.getLogger(\"main\")\n",
    "main_log.setLevel(logging.INFO)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4889c244-5ed9-49f1-bef1-1dd8bbb985d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "def run_question(question):\n",
    "    llm_response = generate(\n",
    "        prompt=question,\n",
    "        model=MODEL,\n",
    "        options={\n",
    "            \"temperature\": 0.5\n",
    "        }\n",
    "    )\n",
    "    main_log.debug(llm_response)\n",
    "    return llm_response.response\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2493ff50-d1e1-491d-bce3-38db1107152e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "print(run_question(\"When is Jettro available?\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Be more specific\n",
    "Ask a better question"
   ],
   "id": "c0b441ca6f3f5f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(run_question(\"I need to make an appointment with Jettro. When is he available?\"))",
   "id": "7d7e43ee773d38f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6437fe34-02a8-4ea3-83e1-8c10031e7cff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": "## Provide a system prompt with some guidelines"
  },
  {
   "cell_type": "markdown",
   "id": "551eae83-6b2a-4e90-a715-c7adc764fa19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "With a system message, you tell the LLM more about the role"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_question_with_system_prompt(question):\n",
    "    llm_response = generate(\n",
    "        prompt=question,\n",
    "        model=MODEL,\n",
    "        options={\n",
    "            \"temperature\": 0.5,\n",
    "        },\n",
    "        system=(\n",
    "            \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "            \"Do not make up availability if you do not know the person's schedule.\"\n",
    "            \"Answer in short sentences, stick to the answer to the question.\"\n",
    "        )\n",
    "    )\n",
    "    main_log.debug(llm_response)\n",
    "    return llm_response.response\n"
   ],
   "id": "120c77eb823ec5b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(run_question_with_system_prompt(\"When is Jettro available?\"))",
   "id": "a6caf0861eed6390",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Provide a context with the required information\n",
    "The way you obtain the context is not important for the LLM."
   ],
   "id": "8c6e14b0cf87e6ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context = (\n",
    "    \"This is the agenda of people in our office:\\n\"\n",
    "    \"- Jettro is available on Monday and Thursday;\\n\"\n",
    "    \"- Joey is available on Tuesday, Thursday, and Friday;\\n\"\n",
    "    \"- Daniel is available from Monday to Thursday.\"\n",
    ")\n",
    "\n",
    "print(run_question_with_system_prompt(f\"{context}\\nWhen is Jettro available?\"))"
   ],
   "id": "d72377b73ac446f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(run_question_with_system_prompt(f\"When is Jettro, Joey and Daniel available together?\"))",
   "id": "27f924a8f26034fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Give the LLM Memory\n",
    "By keeping the messages in a memory, the LLM can keep using the context. Note that we switched from the `generate` function to the `chat` function."
   ],
   "id": "48eddd147d96bd3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_chat(question, messages=None):\n",
    "    _messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "            \"Do not make up availability if you do not know the person's schedule.\"\n",
    "            \"Answer in short sentences, stick to the answer to the question.\"\n",
    "        )},\n",
    "    ]\n",
    "    if messages is not None:\n",
    "        _messages.extend(messages)\n",
    "\n",
    "    _messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    llm_response = chat(\n",
    "        model=MODEL,\n",
    "        options={\n",
    "            \"temperature\": 0.5\n",
    "        },\n",
    "        messages=_messages\n",
    "    )\n",
    "    main_log.debug(llm_response)\n",
    "    _messages.append({\"role\": \"assistant\", \"content\": llm_response.message.content})\n",
    "    return llm_response.message.content, _messages[1:]\n",
    "\n",
    "\n",
    "response_messages = None"
   ],
   "id": "d3cdba168e20875f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response, response_messages = run_chat(question=f\"{context}\\nWhen is Jettro available?\", messages=response_messages)\n",
    "print(response)"
   ],
   "id": "c516593ca29c3664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response, response_messages = run_chat(question=f\"When are Jettro and Joey both available?\", messages=response_messages)\n",
    "print(response)"
   ],
   "id": "a82ede1a9b08f740",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Give the LLM a Tool to ask schedule information\n",
    "By using a Tool, we can have a dynamic context. The Tool is a function that the LLM knows how to call."
   ],
   "id": "e31c41615ba9748b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is the tool\n",
    "def find_person_availability(name: str) -> str:\n",
    "    main_log.info(f\"Finding availability for {name}\")\n",
    "    if name.lower() == \"jettro\":\n",
    "        return \"Jettro is available on Tuesday and Thursday.\"\n",
    "    elif name.lower() == \"daniel\":\n",
    "        return \"Daniel is available on Monday to Thursday.\"\n",
    "    elif name.lower() == \"joey\":\n",
    "        return \"Joey is available on Thursday and Friday.\"\n",
    "    else:\n",
    "        return \"I do not know the availability of this person.\"\n"
   ],
   "id": "e70e9a62ad6b0d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL = \"llama3.2\"\n",
    "\n",
    "def run_chat_with_tool(question, messages=None):\n",
    "    _messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": (\n",
    "            \"You are a scheduling assistant. You help in checking the availability of people.\"\n",
    "            \"Do not make up availability if you do not know the person's schedule.\"\n",
    "            \"Answer in short sentences, stick to the answer to the question.\"\n",
    "        )}]\n",
    "    if messages is not None:\n",
    "        _messages.extend(messages)\n",
    "\n",
    "    if question is not None:\n",
    "        _messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    llm_response = chat(\n",
    "        model=MODEL,\n",
    "        options={\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        messages=_messages,\n",
    "        tools=[find_person_availability]\n",
    "    )\n",
    "    main_log.debug(llm_response)\n",
    "\n",
    "    if llm_response.message.tool_calls:\n",
    "        calls = []\n",
    "        for tool_call in llm_response.message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            argument = tool_call.function.arguments[\"name\"]\n",
    "            calls.append((function_name, argument))\n",
    "        return calls, _messages\n",
    "\n",
    "    _messages.append({\"role\": \"assistant\", \"content\": llm_response.message.content})\n",
    "    return llm_response.message.content, _messages[1:]\n",
    "\n",
    "response_messages = None"
   ],
   "id": "9dbb8036b7331699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response, response_messages = run_chat_with_tool(question=f\"When are Jettro and Joey both available?\", messages=response_messages)\n",
    "print(response)"
   ],
   "id": "1c0222ffa161abea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Call the tool\n",
    "for call in response:\n",
    "    availability = find_person_availability(call[1])\n",
    "    response_messages.append({\"role\": \"tool\", \"content\": availability, \"name\": \"find_person_availability\"})\n",
    "\n",
    "response, response_messages = run_chat_with_tool(question=None, messages=response_messages)\n",
    "print(response)\n"
   ],
   "id": "b9034648410c9232",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
